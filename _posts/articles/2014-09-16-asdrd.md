---
layout: post
title: "Sahani, Linden's ASD/RD"
description: "ASD/RD"
tags: [math, neuro]
categories: articles
latex: true
---

## General model

As is common in regression problems, it is convenient to collect all the stimulus vectors and observed responses into matrices. Thus, we described the input by a matrix \\(X\\), the \\(t\\)th column of which is the input lag-vector \\(x_t\\). Similarly, we collect the outputs into a row vector \\(Y\\), the \\(t\\)th element of which is \\(y_t\\). The first \\(L-1\\) time-steps are dropped to avoid incomplete lag-vectors. Then, assuming independent noise in each time bin, we combine the individual probabilities to give:

\\[ P( Y \mid X, w, \sigma^2 ) = \frac{1}{\sqrt{ \| 2\pi\sigma^2I \| }} \exp\left(-\frac{1}{2}\frac{ (Y - w^TX)(Y - w^TX)^T }{ \sigma^2 }\right) \\]

We now choose the prior distribution on \\(w\\) to be normal with zero mean (having no prior reason to favour either positive or negative weights) and covariance matrix \\(C\\). Then the joint density of \\(Y\\) and \\(w\\) is:

\\[ P( Y, w \mid X, C, \sigma^2 ) = \frac{1}{Z} \exp\left(-\frac{1}{2}\frac{ (Y - w^TX)(Y - w^TX)^T }{ \sigma^2 } - w^TC^{-1}w \right) \\]

where the normalizer \\(Z = \sqrt{ \| 2\pi\sigma^2I \| \| 2 \pi C \| } \\).

Fixing Y to be the observed values, this implies a normal posterior on \\(w\\) with variance \\( \Sigma = \left( \frac{XX^T}{\sigma^2} + C^{-1} \right)^{-1} \\) and mean \\( \mu = \Sigma \frac{XY^T}{\sigma^2} \\). By integrating this normal density in \\(w\\) we obtain an expression for the evidence, \\( \epsilon(C, \sigma^2) \\):

\\[ P( Y \mid X, C, \sigma^2 ) = \sqrt{\frac{ \| 2 \pi \Sigma \| }{ \| 2\pi\sigma^2I \| \| 2 \pi C \| }} \exp\left(-\frac{1}{2} Y \left( \frac{I}{\sigma^2} - \frac{X^T \Sigma X}{\sigma^4} \right) Y^T \right) \\]

and the log evidence, \\( \log \epsilon \\), is thus:

\\[ \log P( Y \mid X, C, \sigma^2 ) = \frac{1}{2} \left( Z_1 - Y \left( \frac{I}{\sigma^2} - \frac{X^T \Sigma X}{\sigma^4} \right) Y^T \right) \\]

where \\( Z_1 = \log( \| 2 \pi \Sigma \| ) - \log( \| 2\pi\sigma^2I \| ) - \log( \| 2 \pi C \| ) \\).

We seek to optimize this evidence with respect to the hyperparameters in \\(C\\), and the noise variance \\(\sigma^2\\). To do this we need the respective gradients. If the covariance matrix contains a parameter \\(\theta\\), then the derivative of the log-evidence with respect to \\(\theta\\) is given by:

\\[ \frac{\partial}{\partial\theta} \log \epsilon  = \frac{1}{2} Tr \left[ \( C - \Sigma - \mu \mu^T \) \frac{\delta}{\delta\theta} C^{-1} \right] \\]

while the gradient in the noise variance is given by:

\\[ \frac{\partial}{\partial\sigma^2} \log \epsilon  = \frac{1}{2\sigma^2} \left[ -T + Tr\( I - \Sigma C^{-1} \) + \frac{1}{\sigma^2}\( Y - \mu^TX \) \( Y - \mu^TX \)^T \right] \\]

where \\(T\\) is the length of \\(Y\\).

## Automatic Relevance Determination (ARD)

Let the parametrized covariance matrix, \\(C\\), depend on a squared distance matrix, \\(\Delta\\), where the \\( (i,j) \\)th element of \\(\Delta\\) gives the squared distance between the weights \\(w_i\\) and \\(w_j\\) in the stimulus space. Then:

\\[ C = \exp \left( -p - \frac{\Delta}{2\delta^2} \right) \\]

where the exponent is taken element by element, and the hyperparameters \\( \( p, \delta \) \\) set the overall scale and the correlation distances along the stimulus dimension, respectively.

## ARD Algorithm

Given a value of \\( \theta = \( \sigma^2, p, \delta \) \\), we can update the posterior covariance and mean using our regularizer, \\( C \\):

\\[ C = \exp \left( -p - \frac{\Delta}{2\delta^2} \right) \\]

\\[ \Sigma = \left( \frac{XX^T}{\sigma^2} + C^{-1} \right)^{-1} \\]

\\[ \mu = \Sigma \frac{XY^T}{\sigma^2} \\]

Knowing the posterior covariance and means allows us to calculate the log-evidence:

\\[ \log \epsilon = \frac{1}{2} \left( Z_1 - Y \left( \frac{I}{\sigma^2} - \frac{X^T \Sigma X}{\sigma^4} \right) Y^T \right) \\]

where \\( Z_1 = \log( \| 2 \pi \Sigma \| ) - \log( \| 2\pi\sigma^2I \| ) - \log( \| 2 \pi C \| ) \\).

And the gradient of the log-evidence:

\\[ \frac{\partial}{\partial\sigma^2} \log \epsilon  = \frac{1}{2\sigma^2} \left[ -T + Tr\( I - \Sigma C^{-1} \) + \frac{1}{\sigma^2}\( Y - \mu^TX \) \( Y - \mu^TX \)^T \right] \\]

\\[ \frac{\partial}{\partial p} \log \epsilon  = \frac{1}{2} Tr \left[ \( C - \Sigma - \mu \mu^T \) C^{-1} \right] \\]

\\[ \frac{\partial}{\partial \delta} \log \epsilon  = - \frac{1}{2} Tr \left[ \( C - \Sigma - \mu \mu^T \) C^{-1} \( C \circ \frac{ \Delta }{\delta^3} \) C^{-1} \right] \\]

where \\( A \circ B\\) is the element-by-element product of \\(A\\) and \\(B\\).

Using these facts, we can now maximize the log-evidence using gradient-descent techniques (along with setting some bounds on our search space, giving us an estimate of the hyperparameters in \\( \theta \\). However, note that some values of \\( \theta \\) will yield matrices \\( \Sigma, C \\) that are _not_ positive definite, so you may have some problems inverting these guys.
